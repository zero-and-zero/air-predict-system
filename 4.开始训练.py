import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½® Matplotlib çš„å­—ä½“ä¸ºæ”¯æŒä¸­æ–‡çš„å­—ä½“
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # ä½¿ç”¨å¾®è½¯é›…é»‘
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# 1. åŠ è½½æ•°æ®
file_path = 'dataset.csv'
if not os.path.exists(file_path):
    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")

try:
    data = pd.read_csv(file_path)
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
except Exception as e:
    raise IOError(f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

# æ£€æŸ¥å¿…éœ€çš„åˆ—
required_columns = {'AQIæŒ‡æ•°', 'AQI_1å¤©å‰', 'PM2.5_1å¤©å‰', 'PM10_1å¤©å‰',
                    'So2_1å¤©å‰', 'No2_1å¤©å‰', 'O3_1å¤©å‰', 'Co_1å¤©å‰'}
missing_columns = required_columns - set(data.columns)
if missing_columns:
    raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}")

# 2. ç‰¹å¾é€‰æ‹©
features = ['AQI_1å¤©å‰', 'PM2.5_1å¤©å‰', 'PM10_1å¤©å‰', 'So2_1å¤©å‰', 'No2_1å¤©å‰', 'O3_1å¤©å‰', 'Co_1å¤©å‰']
X = data[features]
y = data['AQIæŒ‡æ•°']

# å¤„ç†ç¼ºå¤±å€¼
if X.isnull().any().any() or y.isnull().any():
    print("è­¦å‘Š: æ•°æ®ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼Œå°†ä½¿ç”¨ä¸­ä½æ•°å¡«å……")
    X = X.fillna(X.median())
    y = y.fillna(y.median())

# 3. åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)} æ¡è®°å½•")
print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)} æ¡è®°å½•")

# 4. å®šä¹‰æ¨¡å‹åˆ—è¡¨
models = {
    'éšæœºæ£®æ—': {
        'model': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        'desc': "åŸºäºå¤šæ£µå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ æ–¹æ³•"
    },
    'æ¢¯åº¦æå‡': {
        'model': GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1),
        'desc': "é€æ­¥æ„å»ºæ¨¡å‹ï¼Œæ¯ä¸ªæ–°æ¨¡å‹ä¿®æ­£å‰ä¸€ä¸ªæ¨¡å‹çš„è¯¯å·®"
    },
    'çº¿æ€§å›å½’': {
        'model': LinearRegression(),
        'desc': "å‡è®¾ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´ä¸ºçº¿æ€§å…³ç³»çš„ç®€å•æ¨¡å‹"
    },
    'æ”¯æŒå‘é‡æœº': {
        'model': SVR(kernel='rbf', C=1.0, epsilon=0.1),
        'desc': "é€‚ç”¨äºé«˜ç»´ç©ºé—´ä¸­éçº¿æ€§é—®é¢˜çš„ç®—æ³•"
    },
    'Kè¿‘é‚»': {
        'model': KNeighborsRegressor(n_neighbors=5, weights='distance', n_jobs=-1),
        'desc': "åŸºäºé‚»è¿‘æ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹çš„æ–¹æ³•"
    }
}

# åˆ›å»ºç›®å½•ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°ç»“æœ
os.makedirs('models', exist_ok=True)
os.makedirs('evaluation', exist_ok=True)

# 5. è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹
results = []
eval_dfs = []

for name, config in models.items():
    model = config['model']
    description = config['desc']

    print(f"\n{'=' * 60}")
    print(f"è®­ç»ƒ {name} æ¨¡å‹...")
    print(f"æè¿°: {description}")

    try:
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join('models', f'{name}_model.pkl')
        joblib.dump(model, model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}")

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train,
                                    cv=5, scoring='neg_mean_squared_error')
        cv_rmse = np.sqrt(-cv_scores)

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_pred = model.predict(X_test)

        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        accuracy = (abs(y_pred - y_test) <= 30).mean()

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_df = pd.DataFrame({
            'çœŸå®å€¼': y_test,
            'é¢„æµ‹å€¼': y_pred,
            'è¯¯å·®': y_pred - y_test
        })
        eval_path = os.path.join('evaluation', f'{name}_predictions.csv')
        eval_df.to_csv(eval_path, index=False)
        print(f"ğŸ“Š é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ {eval_path}")

        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        model_results = {
            'æ¨¡å‹': name,
            'æè¿°': description,
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'å‡†ç¡®ç‡(Â±30)': accuracy,
            'äº¤å‰éªŒè¯RMSEå‡å€¼': cv_rmse.mean(),
            'äº¤å‰éªŒè¯RMSEæ ‡å‡†å·®': cv_rmse.std()
        }
        results.append(model_results)

        # æ‰“å°å½“å‰æ¨¡å‹ç»“æœ
        print(f"æ¨¡å‹æ€§èƒ½:")
        print(f"- MAE: {mae:.2f}")
        print(f"- RMSE: {rmse:.2f}")
        print(f"- RÂ²: {r2:.4f}")
        print(f"- å‡†ç¡®ç‡(è¯¯å·®â‰¤30): {accuracy:.2%}")
        print(f"- äº¤å‰éªŒè¯RMSE: {cv_rmse.mean():.2f} Â± {cv_rmse.std():.2f}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒæ¨¡å‹ {name} æ—¶å‡ºé”™: {str(e)}")
        results.append({
            'æ¨¡å‹': name,
            'æè¿°': description,
            'MAE': None,
            'RMSE': None,
            'R2': None,
            'å‡†ç¡®ç‡(Â±30)': None,
            'äº¤å‰éªŒè¯RMSEå‡å€¼': None,
            'äº¤å‰éªŒè¯RMSEæ ‡å‡†å·®': None,
            'é”™è¯¯': str(e)
        })

# 6. è¾“å‡ºæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
results_df = pd.DataFrame(results)
print("\n" + "=" * 60)
print("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
print(results_df.drop(columns=['æè¿°']).to_string(index=False))

# ä¿å­˜è¯¦ç»†ç»“æœ
results_path = os.path.join('evaluation', 'model_performance_summary.csv')
results_df.to_csv(results_path, index=False)
print(f"\nğŸ“ˆ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ {results_path}")


# 7. å¯è§†åŒ–ç»“æœ - ä¿®å¤é”™è¯¯å¹¶ä¼˜åŒ–
# 7.1 æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
def plot_metrics_comparison():
    """ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒå›¾"""
    plt.figure(figsize=(14, 10))

    # åˆ›å»ºå­å›¾
    metrics = ['MAE', 'RMSE', 'R2', 'å‡†ç¡®ç‡(Â±30)']
    titles = ['å¹³å‡ç»å¯¹è¯¯å·®(MAE)', 'å‡æ–¹æ ¹è¯¯å·®(RMSE)', 'å†³å®šç³»æ•°(RÂ²)', 'å‡†ç¡®ç‡(è¯¯å·®â‰¤30)']

    for i, metric in enumerate(metrics):
        ax = plt.subplot(2, 2, i + 1)
        # ä½¿ç”¨æ¡å½¢å›¾è€Œä¸æ˜¯barplot
        x = range(len(results_df))
        values = results_df[metric]

        # è·³è¿‡æ— æ•ˆå€¼
        valid_mask = values.notna()
        valid_df = results_df[valid_mask]
        valid_values = valid_df[metric]

        if len(valid_values) > 0:
            bars = ax.bar(x, values, color=plt.cm.tab10.colors[:len(values)])

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                if not pd.isna(height):
                    ax.text(bar.get_x() + bar.get_width() / 2., height,
                            f'{height:.2f}' if metric != 'R2' else f'{height:.4f}',
                            ha='center', va='bottom')

            # è®¾ç½®xè½´æ ‡ç­¾
            ax.set_xticks(x)
            ax.set_xticklabels(results_df['æ¨¡å‹'], rotation=45)
            ax.set_title(titles[i])
            ax.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.savefig(os.path.join('evaluation', 'performance_metrics.png'))
    plt.close()
    print("âœ… æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒå›¾å·²ä¿å­˜")


# 7.2 äº¤å‰éªŒè¯ç»“æœæ¯”è¾ƒ - ä¿®å¤é”™è¯¯
def plot_cv_comparison():
    """ç»˜åˆ¶äº¤å‰éªŒè¯ç»“æœæ¯”è¾ƒå›¾"""
    plt.figure(figsize=(10, 6))

    # ç­›é€‰æœ‰æ•ˆæ•°æ®
    valid_mask = results_df['äº¤å‰éªŒè¯RMSEå‡å€¼'].notna() & results_df['äº¤å‰éªŒè¯RMSEæ ‡å‡†å·®'].notna()
    valid_df = results_df[valid_mask]

    if len(valid_df) > 0:
        x = range(len(valid_df))
        means = valid_df['äº¤å‰éªŒè¯RMSEå‡å€¼']
        stds = valid_df['äº¤å‰éªŒè¯RMSEæ ‡å‡†å·®']

        # ä½¿ç”¨æ¡å½¢å›¾è€Œä¸æ˜¯barplot
        bars = plt.bar(x, means, yerr=stds, capsize=5, color=plt.cm.tab10.colors[:len(means)])

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean, std in zip(bars, means, stds):
            plt.text(bar.get_x() + bar.get_width() / 2., mean + std + 0.5,
                     f'{mean:.2f}Â±{std:.2f}', ha='center', va='bottom')

        plt.xticks(x, valid_df['æ¨¡å‹'], rotation=45)
        plt.title('äº¤å‰éªŒè¯RMSEæ¯”è¾ƒï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰')
        plt.ylabel('RMSE')
        plt.grid(True, linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.savefig(os.path.join('evaluation', 'cross_validation.png'))
        plt.close()
        print("âœ… äº¤å‰éªŒè¯ç»“æœæ¯”è¾ƒå›¾å·²ä¿å­˜")
    else:
        print("âš ï¸ æ— æœ‰æ•ˆçš„äº¤å‰éªŒè¯æ•°æ®å¯ç»˜åˆ¶å›¾è¡¨")


# 7.3 è¯¯å·®åˆ†å¸ƒå¯è§†åŒ–
def plot_error_distributions():
    """ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾"""
    plt.figure(figsize=(14, 10))

    for i, name in enumerate(models.keys()):
        ax = plt.subplot(2, 3, i + 1)

        eval_path = os.path.join('evaluation', f'{name}_predictions.csv')
        if os.path.exists(eval_path):
            try:
                eval_df = pd.read_csv(eval_path)
                errors = eval_df['è¯¯å·®'].dropna()

                if len(errors) > 0:
                    # ç»˜åˆ¶ç›´æ–¹å›¾å’ŒKDEæ›²çº¿
                    sns.histplot(errors, bins=30, kde=True, color=plt.cm.tab10(i))

                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    mean_err = errors.mean()
                    std_err = errors.std()
                    ax.axvline(mean_err, color='r', linestyle='--')
                    ax.text(0.95, 0.95, f'å‡å€¼: {mean_err:.2f}\næ ‡å‡†å·®: {std_err:.2f}',
                            transform=ax.transAxes, ha='right', va='top',
                            bbox=dict(facecolor='white', alpha=0.8))

                    ax.set_title(f'{name}æ¨¡å‹é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
                    ax.set_xlabel('è¯¯å·®ï¼ˆé¢„æµ‹å€¼ - çœŸå®å€¼ï¼‰')
                    ax.set_ylabel('é¢‘ç‡')
                    ax.grid(True, linestyle='--', alpha=0.5)
            except Exception as e:
                print(f"âŒ æ— æ³•åŠ è½½ {name} çš„é¢„æµ‹ç»“æœ: {str(e)}")

    plt.tight_layout()
    plt.savefig(os.path.join('evaluation', 'error_distributions.png'))
    plt.close()
    print("âœ… è¯¯å·®åˆ†å¸ƒå›¾å·²ä¿å­˜")


# æ‰§è¡Œå¯è§†åŒ–å‡½æ•°
plot_metrics_comparison()
plot_cv_comparison()
plot_error_distributions()

print("\nè®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹å®Œæˆï¼")